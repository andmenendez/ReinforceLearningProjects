# Reinforcement Learning Projects for CS7642
## Implementing TD-Lambda to solve a "Random Walk Problem" [link]()
Machine learning involves a system that can adapt to its own experience in order to predict its own future. This means that such a system would be able to accurately evaluate its current state and thereby change its state to maximize its returns. Richard S. Sutton’s paper 1 on “Learning to Predict by the methods of Temporal Differences” introduces a novel method to prediction learning that gains experience directly from temporal predictions without supervision. These intermediary temporal steps can be unique to each problem and can be weighted at different values depending on the selected approach. The problem described below ventures to explore the optimal weights for the various parameters that can be used to uniquely adapt the TD method to a problem. This problem is from Sutton’s paper 1 and looks to replicate the same results in order to explore the implications and the design.

## Exploring ML Game Theory 
It’s a saturday morning and the dew on the windows drips as my espresso presses fresh aromas across my apartment. My cat stretches her from tail to claw as I sit down to immerse myself in the dark notes of my coffee. It is at this time when I consider the day and the variety of things I could do and should do. Defined by my own values, I choose as wisely as I can. Each idea, rich with subconscious calculations, approximates my understanding of my actions to the objective value to those actions. It is here where the game of life evolves as it flourishes from our futile attempts to bootstrap our limited experiences to reach even the remoteness regions of truth. Reinforcement learning has brought forth an objective wave of understanding to this very human process of identifying the best possible action within the world of Game Theory.

## Using a DQN to solve the OpenAI Lunar Lander
Markov Chains can be leveraged to create an agent that can optimally learn through iterative attempts and maximize long term rewards. A Deep Q-Network (DQN) algorithm can be used to solve this MDP by solving for an optimal policy. The DQN network anchors itself on a neural network to efficiently store Q-values of a continuous state-space and thereby develop an optimal policy. This avoids massive tables of Q-values and allows the agent to maximize its learning on a reduced number of experiences. Gradient descent allows the agent to reliably converge on near optimal q-values through the updating of weights within the neural network. This algorithm is driven by reinforcement learning theory where the agent can learn after every action taken. Several hyperparameters play a heavy role in how the agent learns. This is not only with respect to the values of these parameters, but how and when they are updated and controlled. The following discussion looks into the role of this algorithm within the paradigm of reinforcement learning.

## Implementing TD-Lambda to solve a "Random Walk Problem" [link]()
Machine learning involves a system that can adapt to its own experience in order to predict its own future. This means that such a system would be able to accurately evaluate its current state and thereby change its state to maximize its returns. Richard S. Sutton’s paper 1 on “Learning to Predict by the methods of Temporal Differences” introduces a novel method to prediction learning that gains experience directly from temporal predictions without supervision. These intermediary temporal steps can be unique to each problem and can be weighted at different values depending on the selected approach. The problem described below ventures to explore the optimal weights for the various parameters that can be used to uniquely adapt the TD method to a problem. This problem is from Sutton’s paper 1 and looks to replicate the same results in order to explore the implications and the design.
